{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp==1.7.3 in /opt/conda/lib/python3.7/site-packages (1.7.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install spark-nlp==1.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StringType, DataType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession\n",
    "pd.set_option('max_colwidth', 800)\n",
    "import matplotlib\n",
    "from pyspark.sql.types import TimestampType, StructType, StructField, StringType, FloatType, IntegerType, LongType\n",
    "from pyspark.sql import types as T\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\" Sparkify Localy\") \\\n",
    "        .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.8.2\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f23e51162d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = spark_session()\n",
    "# get parameters\n",
    "spark.sparkContext.getConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Song_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = \"./song_data/A/A/A/*.json\"\n",
    "input_song = \"./song_data/*/*/*/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data):\n",
    "    '''\n",
    "    process song data\n",
    "    \n",
    "    return df_song\n",
    "    '''\n",
    "    # read  data file\n",
    "    song_schema = StructType([\n",
    "        StructField(\"num_songs\", IntegerType()),\n",
    "        StructField(\"artist_id\", StringType()),\n",
    "        StructField(\"artist_latitude\", FloatType()),\n",
    "        StructField(\"artist_longitude\", FloatType()),\n",
    "        StructField(\"artist_location\", StringType()),\n",
    "        StructField(\"artist_name\", StringType()),\n",
    "        StructField(\"song_id\", StringType()),\n",
    "        StructField(\"title\", StringType()),\n",
    "        StructField(\"duration\", FloatType()),\n",
    "        StructField(\"year\", IntegerType())\n",
    "    ])\n",
    "    \n",
    "    df_song = spark.read.json(input_data, schema = song_schema)\n",
    "    print('DataFrame rows: %d' % df_song.count())\n",
    "    df_song.printSchema()\n",
    "    print('DataFrame schema: %s' % df_song)\n",
    "    return df_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame rows: 71\n",
      "root\n",
      " |-- num_songs: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: float (nullable = true)\n",
      " |-- artist_longitude: float (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: float (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "DataFrame schema: DataFrame[num_songs: int, artist_id: string, artist_latitude: float, artist_longitude: float, artist_location: string, artist_name: string, song_id: string, title: string, duration: float, year: int]\n"
     ]
    }
   ],
   "source": [
    "df_song = process_song_data(spark, input_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------------+----------------+-----------------+----------------------------------------------------------------------------------------------+------------------+-------------------------------+---------+----+\n",
      "|num_songs|artist_id         |artist_latitude|artist_longitude|artist_location  |artist_name                                                                                   |song_id           |title                          |duration |year|\n",
      "+---------+------------------+---------------+----------------+-----------------+----------------------------------------------------------------------------------------------+------------------+-------------------------------+---------+----+\n",
      "|1        |ARDR4AC1187FB371A1|null           |null            |                 |Montserrat Caballé;Placido Domingo;Vicente Sardinero;Judith Blegen;Sherrill Milnes;Georg Solti|SOBAYLL12A8C138AF9|Sono andati? Fingevo di dormire|511.16364|0   |\n",
      "|1        |AREBBGV1187FB523D2|null           |null            |Houston, TX      |Mike Jones (Featuring CJ_ Mello & Lil' Bran)                                                  |SOOLYAZ12A6701F4A6|Laws Patrolling (Album Version)|173.66159|0   |\n",
      "|1        |ARMAC4T1187FB3FA4C|40.82624       |-74.47995       |Morris Plains, NJ|The Dillinger Escape Plan                                                                     |SOBBUGU12A8C13E95D|Setting Fire to Sleeping Giants|207.77751|2004|\n",
      "+---------+------------------+---------------+----------------+-----------------+----------------------------------------------------------------------------------------------+------------------+-------------------------------+---------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_song.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions use in all the programm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parquet(parquet_path):\n",
    "    ! ls 2>&1 -lh $parquet_path | head -10\n",
    "    ! echo 'Parquet Files:' $(ls | wc -l)\n",
    "    table_parquet = spark.read.parquet(parquet_path)\n",
    "    print('DataFrame rows: %d' % table_parquet.count())\n",
    "    print('DataFrame schema: %s' % table_parquet)\n",
    "    table_parquet.show(10, False)\n",
    "    return table_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_timestamp(df):\n",
    "    # convert timestamps to date time from epoch time so we can get hour of the day\n",
    "    get_timestamp = F.udf(lambda x: datetime.fromtimestamp(x/1000), T.TimestampType())\n",
    "    # add a new column `formated_ts` in our dataframe\n",
    "    df_log_copy = df.withColumn(\"formated_ts\", get_timestamp(df.ts))\n",
    "    df_formated = df_log_copy.dropna(subset='ts')\n",
    "    return df_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_songs_table(df):\n",
    "    table = df_song \\\n",
    "        .select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\") \\\n",
    "        .filter('song_id != \"\" and title != \"\" and artist_id != \"\"') \\\n",
    "        .sort(\"song_id\") \\\n",
    "        .drop_duplicates(['song_id'])\n",
    "    return(table)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(table, parquet_path):\n",
    "    table.write.partitionBy(\"year\", \"artist_id\").parquet(parquet_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 84K\n",
      "-rw-r--r--  1 anthelix users    0 Mar 22 02:31 _SUCCESS\n",
      "drwxr-xr-x 43 anthelix users 4.0K Mar 22 02:31 year=0\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 22 02:31 year=1961\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 22 02:31 year=1964\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 22 02:31 year=1969\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 22 02:31 year=1972\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 22 02:31 year=1982\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 22 02:31 year=1984\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 22 02:31 year=1985\n",
      "Parquet Files: 17\n",
      "DataFrame rows: 71\n",
      "DataFrame schema: DataFrame[song_id: string, title: string, duration: float, year: int, artist_id: string]\n",
      "+------------------+----------------------------------------------------+---------+----+------------------+\n",
      "|song_id           |title                                               |duration |year|artist_id         |\n",
      "+------------------+----------------------------------------------------+---------+----+------------------+\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand In Mine [Live At Royal Albert Hall]|43.36281 |2000|ARPBNLO1187FB3D52F|\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife Is Running Around On Me (Taco Hell) |186.48772|2005|ARDNS031187B9924F0|\n",
      "|SODREIN12A58A7F2E5|A Whiter Shade Of Pale (Live @ Fillmore West)       |326.00772|0   |ARLTWXK1187FB5A3F8|\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (Ordinary Day Album Version)         |267.7024 |0   |ARKFYS91187B98E58F|\n",
      "|SOWQTQZ12A58A7B63E|Streets On Fire (Explicit Album Version)            |279.97995|0   |ARPFHN61187FB575F6|\n",
      "|SOUDSGM12AC9618304|Insatiable (Instrumental Version)                   |266.39627|0   |ARNTLGG11E2835DDB9|\n",
      "|SOPEGZN12AB0181B3D|Get Your Head Stuck On Your Neck                    |45.66159 |0   |AREDL271187FB40F44|\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fingevo di dormire                     |511.16364|0   |ARDR4AC1187FB371A1|\n",
      "|SOOLYAZ12A6701F4A6|Laws Patrolling (Album Version)                     |173.66159|0   |AREBBGV1187FB523D2|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to Sleeping Giants                     |207.77751|2004|ARMAC4T1187FB3FA4C|\n",
      "+------------------+----------------------------------------------------+---------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[song_id: string, title: string, duration: float, year: int, artist_id: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "songs_table = create_songs_table(df_song)\n",
    "parquet_path = 'output/songs_table'\n",
    "write_parquet(songs_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#newRow = spark.createDataFrame([('','','F', 0, 0)])\n",
    "#df1 = songs_table.union(newRow).sort(\"song_id\").\n",
    "#df1.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "songs_table.write.partitionBy(\"year\", \"artist_id\").parquet('output/songs_table', mode = 'overwrite')\n",
    "print('DataFrame rows: %d' % songs_table.count())\n",
    "print('DataFrame schema: %s' % songs_table)\n",
    "songs_table.sort('title').show(10, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "songs_table_parquet = spark.read.parquet('output/songs_table')\n",
    "print('DataFrame rows: %d' % songs_table_parquet.count())\n",
    "print('DataFrame schema: %s' % songs_table_parquet)\n",
    "songs_table_parquet.select('song_id', 'title', 'artist_id', 'year', 'duration') \\\n",
    "    .sort('title') \\\n",
    "    .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Artists Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artists_table(df):\n",
    "    table = df \\\n",
    "        .selectExpr(\"artist_id\", \"artist_name as name\", \"artist_location as location\", \"artist_latitude as latitude\", \"artist_longitude as longitude\") \\\n",
    "        .filter('artist_id != \"\" and name != \"\"') \\\n",
    "        .sort(\"artist_id\") \\\n",
    "        .drop_duplicates(['artist_id'])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(table, parquet_path):\n",
    "    table.write.parquet(parquet_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 276K\n",
      "-rw-r--r-- 1 anthelix users 1.3K Mar 22 02:31 part-00000-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.5K Mar 22 02:31 part-00001-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.6K Mar 22 02:31 part-00002-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.7K Mar 22 02:31 part-00003-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.5K Mar 22 02:31 part-00004-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.4K Mar 22 02:31 part-00005-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.5K Mar 22 02:31 part-00006-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.6K Mar 22 02:31 part-00007-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.5K Mar 22 02:31 part-00008-1fb9c16c-87ae-4cf5-8717-cc9324c6ae77-c000.snappy.parquet\n",
      "Parquet Files: 17\n",
      "DataFrame rows: 69\n",
      "DataFrame schema: DataFrame[artist_id: string, name: string, location: string, latitude: float, longitude: float]\n",
      "+------------------+----------------------------------------------------------------------------------------------+-----------------------------+--------+---------+\n",
      "|artist_id         |name                                                                                          |location                     |latitude|longitude|\n",
      "+------------------+----------------------------------------------------------------------------------------------+-----------------------------+--------+---------+\n",
      "|ARDR4AC1187FB371A1|Montserrat Caballé;Placido Domingo;Vicente Sardinero;Judith Blegen;Sherrill Milnes;Georg Solti|                             |null    |null     |\n",
      "|AREBBGV1187FB523D2|Mike Jones (Featuring CJ_ Mello & Lil' Bran)                                                  |Houston, TX                  |null    |null     |\n",
      "|ARMAC4T1187FB3FA4C|The Dillinger Escape Plan                                                                     |Morris Plains, NJ            |40.82624|-74.47995|\n",
      "|ARNF6401187FB57032|Sophie B. Hawkins                                                                             |New York, NY [Manhattan]     |40.79086|-73.96644|\n",
      "|AROUOZZ1187B9ABE51|Willie Bobo                                                                                   |New York, NY [Spanish Harlem]|40.79195|-73.94512|\n",
      "|AR9AWNF1187B9AB0B4|Kenny G featuring Daryl Hall                                                                  |Seattle, Washington USA      |null    |null     |\n",
      "|ARI2JSK1187FB496EF|Nick Ingman;Gavyn Wright                                                                      |London, England              |51.50632|-0.12714 |\n",
      "|AR10USD1187B99F3F1|Tweeterfriendly Music                                                                         |Burlington, Ontario, Canada  |null    |null     |\n",
      "|ARD842G1187B997376|Blue Rodeo                                                                                    |Toronto, Ontario, Canada     |43.64856|-79.38533|\n",
      "|AR0RCMP1187FB3F427|Billie Jo Spears                                                                              |Beaumont, TX                 |30.08615|-94.10158|\n",
      "+------------------+----------------------------------------------------------------------------------------------+-----------------------------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[artist_id: string, name: string, location: string, latitude: float, longitude: float]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "artists_table = create_artists_table(df_song)\n",
    "parquet_path = 'output/artists_table'\n",
    "write_parquet(artists_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "print('DataFrame rows: %d' % artists_table.count())\n",
    "print('DataFrame schema: %s' % artists_table)\n",
    "artists_table.sort('artist_id').show(10, False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "artists_table.write.parquet('output/artists_table', mode = 'overwrite')\n",
    "! ls 2>&1 -lh output/artists_table | head -10\n",
    "! echo 'Parquet Files:' $(ls | wc -l)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "artists_table_parquet = spark.read.parquet('output/artists_table')\n",
    "print('DataFrame rows: %d' % artists_table_parquet.count())\n",
    "print('DataFrame schema: %s' % artists_table_parquet)\n",
    "artists_table_parquet.select('artist_id', 'name', 'location', 'latitude', 'longitude') \\\n",
    "    .sort('artist_id') \\\n",
    "    .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_log = \"./log_data/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_log):\n",
    "    '''\n",
    "    process log data\n",
    "    \n",
    "    return df_log, users_table, time_table, songplays_table\n",
    "    '''\n",
    "    # read log data file\n",
    "    log_schema = StructType([\n",
    "        StructField(\"artist\", StringType()),\n",
    "        StructField(\"auth\", StringType()),\n",
    "        StructField(\"firstName\", StringType()),\n",
    "        StructField(\"gender\", StringType()),\n",
    "        StructField(\"itemInSession\", IntegerType()),\n",
    "        StructField(\"lastName\", StringType()),\n",
    "        StructField(\"length\", FloatType()),    \n",
    "        StructField(\"level\", StringType()),\n",
    "        StructField(\"location\", StringType()),\n",
    "        StructField(\"method\", StringType()),\n",
    "        StructField(\"page\", StringType()),\n",
    "        StructField(\"registration\", FloatType()),\n",
    "        StructField(\"sessionId\", StringType()),\n",
    "        StructField(\"song\", StringType()),\n",
    "        StructField(\"status\", IntegerType()),\n",
    "        StructField(\"ts\", LongType()),\n",
    "        StructField(\"userAgent\", StringType()),\n",
    "        StructField(\"userId\", StringType())\n",
    "    ])\n",
    "    \n",
    "    df_log_raw = spark.read.json(input_log, schema = log_schema)\n",
    "    df_log_next = df_log_raw.filter(\"page='NextSong'\")\n",
    "    df_log_clean=clean_timestamp(df_log_next)\n",
    "    \n",
    "    print('DataFrame rows: %d' % df_log_clean.count())\n",
    "    df_log_clean.printSchema()\n",
    "    print('DataFrame schema: %s' % df_log_clean)\n",
    "    return df_log_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame rows: 6820\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: float (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: float (nullable = true)\n",
      " |-- sessionId: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- formated_ts: timestamp (nullable = true)\n",
      "\n",
      "DataFrame schema: DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: int, lastName: string, length: float, level: string, location: string, method: string, page: string, registration: float, sessionId: string, song: string, status: int, ts: bigint, userAgent: string, userId: string, formated_ts: timestamp]\n"
     ]
    }
   ],
   "source": [
    "df_log_clean = process_log_data(spark, input_log)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_log_clean.createOrReplaceTempView('tmp_log')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sql_query = \"\"\" SELECT page, count(*) as count\n",
    "                 FROM tmp_log\n",
    "                 GROUP BY page\n",
    "                 ORDER BY count DESC\n",
    "\n",
    "\"\"\"\n",
    "df_page = spark.sql(sql_query)\n",
    "df_page.show(10, False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_log_clean = df_log.filter(\"page='NextSong'\")\n",
    "df_log_clean.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+-------------+--------+--------+-----+----------------------------------+------+--------+-------------+---------+-------------+------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+------+-----------------------+\n",
      "|artist  |auth     |firstName|gender|itemInSession|lastName|length  |level|location                          |method|page    |registration |sessionId|song         |status|ts           |userAgent                                                                                                                                |userId|formated_ts            |\n",
      "+--------+---------+---------+------+-------------+--------+--------+-----+----------------------------------+------+--------+-------------+---------+-------------+------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+------+-----------------------+\n",
      "|Harmonia|Logged In|Ryan     |M     |0            |Smith   |655.7775|free |San Jose-Sunnyvale-Santa Clara, CA|PUT   |NextSong|1.54101665E12|583      |Sehr kosmisch|200   |1542241826796|\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"|26    |2018-11-15 00:30:26.796|\n",
      "+--------+---------+---------+------+-------------+--------+--------+-----+----------------------------------+------+--------+-------------+---------+-------------+------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_log_clean.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Harmonia', auth='Logged In', firstName='Ryan', gender='M', itemInSession=0, lastName='Smith', length=655.7775268554688, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016649728.0, sessionId='583', song='Sehr kosmisch', status=200, ts=1542241826796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26', formated_ts=datetime.datetime(2018, 11, 15, 0, 30, 26, 796000)),\n",
       " Row(artist='The Prodigy', auth='Logged In', firstName='Ryan', gender='M', itemInSession=1, lastName='Smith', length=260.07464599609375, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016649728.0, sessionId='583', song='The Big Gundown', status=200, ts=1542242481796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26', formated_ts=datetime.datetime(2018, 11, 15, 0, 41, 21, 796000))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_clean.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create users table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_users_table(df):\n",
    "    users_table = df \\\n",
    "        .filter('level != \"\"') \\\n",
    "        .orderBy(\"ts\", ascending = False) \\\n",
    "        .coalesce(1)\\\n",
    "        .selectExpr(\"cast(userId as Long) user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender\", \"level\")\\\n",
    "        .drop_duplicates(subset = ['user_id'])\n",
    "    return users_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(table, parquet_path):\n",
    "    table.write.parquet(parquet_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "-rw-r--r-- 1 anthelix users 3.5K Mar 22 02:31 part-00000-0868f3d1-8577-4618-9626-3bbb5198df65-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users    0 Mar 22 02:31 _SUCCESS\n",
      "Parquet Files: 17\n",
      "DataFrame rows: 96\n",
      "DataFrame schema: DataFrame[user_id: bigint, first_name: string, last_name: string, gender: string, level: string]\n",
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|2      |Jizelle   |Benjamin |F     |free |\n",
      "|3      |Isaac     |Valdez   |M     |free |\n",
      "|4      |Alivia    |Terrell  |F     |free |\n",
      "|5      |Elijah    |Davis    |M     |free |\n",
      "|6      |Cecilia   |Owens    |F     |free |\n",
      "|7      |Adelyn    |Jordan   |F     |free |\n",
      "|8      |Kaylee    |Summers  |F     |free |\n",
      "|9      |Wyatt     |Scott    |M     |free |\n",
      "|10     |Sylvie    |Cruz     |F     |free |\n",
      "|11     |Christian |Porter   |F     |free |\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: bigint, first_name: string, last_name: string, gender: string, level: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "users_table = create_users_table(df_log_clean)\n",
    "parquet_path = 'output/users_table'\n",
    "write_parquet(users_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#newRow = spark.createDataFrame([(1,'Alk','Dhl','F', '')])\n",
    "#print(newRow.show())\n",
    "#df1 = users_table.union(newRow)\n",
    "print('DataFrame rows: %d' % users_table.count())\n",
    "print('DataFrame schema: %s' % users_table)\n",
    "users_table.sort(\"user_id\").show(10, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "users_table.write.parquet('output/users_table', mode = 'overwrite')\n",
    "! ls 2>&1 -lh output/users_table | head -10\n",
    "! echo 'Parquet Files:' $(ls | wc -l)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "users_table_parquet = spark.read.parquet('output/users_table')\n",
    "print('DataFrame rows: %d' % users_table_parquet.count())\n",
    "print('DataFrame schema: %s' % users_table_parquet)\n",
    "users_table_parquet.select('user_id', 'first_name', 'last_name', 'gender', 'level') \\\n",
    "    .sort('user_id') \\\n",
    "    .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Time table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert timestamps to date time from epoch time so we can get hour of the day\n",
    "get_timestamp = F.udf(lambda x: datetime.fromtimestamp(x/1000), T.TimestampType()) \n",
    "# add a new column `hour` in our dataframe\n",
    "df_log_copy = df_log_next.withColumn(\"timestamp\", get_timestamp(df_log_next.ts))\n",
    "df_formated = df_log_copy.dropna(subset='ts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_table(df):\n",
    "    time_df = df.select(\n",
    "        col('formated_ts').alias(\"start_time\"),\n",
    "        hour(col('formated_ts')).alias('hour'),\n",
    "        dayofmonth(col('formated_ts')).alias('day'),\n",
    "        weekofyear(col('formated_ts')).alias('week'),\n",
    "        month(col('formated_ts')).alias('month'),\n",
    "        year(col('formated_ts')).alias('year')    \n",
    "    ).drop_duplicates(['start_time'])\n",
    "    time_table = time_df.withColumn('hour', F.hour('start_time')) \\\n",
    "                    .withColumn('day', F.dayofmonth('start_time')) \\\n",
    "                    .withColumn('year', F.year('start_time')) \\\n",
    "                    .withColumn('week', F.weekofyear('start_time')) \\\n",
    "                    .withColumn('month', F.month('start_time')) \\\n",
    "                    .withColumn('weekday', F.dayofweek('start_time').cast(\"string\"))\n",
    "    return time_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(table, parquet_path):\n",
    "    table.write.partitionBy(['year', 'month']).parquet(parquet_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "-rw-r--r-- 1 anthelix users    0 Mar 22 02:31 _SUCCESS\n",
      "drwxr-xr-x 3 anthelix users 4.0K Mar 22 02:31 year=2018\n",
      "Parquet Files: 17\n",
      "DataFrame rows: 6813\n",
      "DataFrame schema: DataFrame[start_time: timestamp, hour: int, day: int, week: int, weekday: string, year: int, month: int]\n",
      "+-----------------------+----+---+----+-------+----+-----+\n",
      "|start_time             |hour|day|week|weekday|year|month|\n",
      "+-----------------------+----+---+----+-------+----+-----+\n",
      "|2018-11-15 16:36:45.796|16  |15 |46  |5      |2018|11   |\n",
      "|2018-11-15 19:02:26.796|19  |15 |46  |5      |2018|11   |\n",
      "|2018-11-21 15:26:35.796|15  |21 |47  |4      |2018|11   |\n",
      "|2018-11-21 17:55:11.796|17  |21 |47  |4      |2018|11   |\n",
      "|2018-11-21 18:49:29.796|18  |21 |47  |4      |2018|11   |\n",
      "|2018-11-14 09:29:50.796|9   |14 |46  |4      |2018|11   |\n",
      "|2018-11-28 18:39:53.796|18  |28 |48  |4      |2018|11   |\n",
      "|2018-11-28 22:46:07.796|22  |28 |48  |4      |2018|11   |\n",
      "|2018-11-05 12:26:13.796|12  |5  |45  |2      |2018|11   |\n",
      "|2018-11-05 14:39:43.796|14  |5  |45  |2      |2018|11   |\n",
      "+-----------------------+----+---+----+-------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[start_time: timestamp, hour: int, day: int, week: int, weekday: string, year: int, month: int]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "time_table = create_time_table(df_log_clean)\n",
    "parquet_path = 'output/time_table'\n",
    "write_parquet(time_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time_table.write.partitionBy(['year', 'month']).parquet('output/time_table', mode = 'overwrite')\n",
    "! ls 2>&1 -lh output/time_table | head -10\n",
    "! echo 'Parquet Files:' $(ls | wc -l)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time_table_parquet = spark.read.parquet('output/time_table')\n",
    "print('DataFrame rows: %d' % time_table_parquet.count())\n",
    "print('DataFrame schema: %s' % time_table_parquet)\n",
    "time_table_parquet.select('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday') \\\n",
    "    .sort('start_time') \\\n",
    "    .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the songplays fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time schema: DataFrame[start_time: timestamp, hour: int, day: int, week: int, month: int, year: int, weekday: string]\n",
      "Users schema: DataFrame[user_id: bigint, first_name: string, last_name: string, gender: string, level: string]\n",
      "LOG_CLEAN schema: DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: int, lastName: string, length: float, level: string, location: string, method: string, page: string, registration: float, sessionId: string, song: string, status: int, ts: bigint, userAgent: string, userId: string, formated_ts: timestamp]\n",
      "DF_SONG: DataFrame[num_songs: int, artist_id: string, artist_latitude: float, artist_longitude: float, artist_location: string, artist_name: string, song_id: string, title: string, duration: float, year: int]\n"
     ]
    }
   ],
   "source": [
    "#time_table.createOrReplaceTempView('tmp_time')\n",
    "print('Time schema: %s' % time_table)\n",
    "#songs_table.createOrReplaceTempView('tmp_songs')\n",
    "#print('Songs schema: %s' % songs_table)\n",
    "#artists_table.createOrReplaceTempView('tmp_artists')\n",
    "#print('Artists schema: %s' % artists_table)\n",
    "#users_table.createOrReplaceTempView('tmp_users')\n",
    "print('Users schema: %s' % users_table)\n",
    "df_log_clean.createOrReplaceTempView('tmp_log')\n",
    "print('LOG_CLEAN schema: %s' % df_log_clean)\n",
    "df_song.createOrReplaceTempView('tmp_song')\n",
    "print('DF_SONG: %s' % df_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_songplays_table(tl, ts,tt):\n",
    "    tl = df_log_clean.alias('tl')\n",
    "    ts = df_song.alias('ts')\n",
    "    \n",
    "    inner_join = tl.join(ts, ((tl.artist == ts.artist_name) & (tl.artist == ts.artist_name)), how='inner')    \n",
    "    songplays = inner_join.withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "    \n",
    "    songplays_table = songplays.selectExpr(\"songplay_id\",\n",
    "                                    \"formated_ts as start_time\",\n",
    "                                    \"cast(userId as Long) user_id\",\n",
    "                                    \"level\",\n",
    "                                    \"song_id\",\n",
    "                                    \"artist_id\",\n",
    "                                    \"sessionId as session_id\",\n",
    "                                    \"location\",\n",
    "                                    \"userAgent as user_agent\",\n",
    "                                    \"year('formated_ts') as year\",\n",
    "                                    \"month('formated_ts') as month\")\n",
    "    return songplays_table\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inner_join = tl \\\n",
    "    .join(ts, ((tl.artist == ts.artist_name) & (tl.artist == ts.artist_name))) \\\n",
    "    .join(tt, tl.timestamp == tt.start_time)\n",
    "songplays = inner_join.withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "songplays_table = songplays.select( \"songplay_id\",\n",
    "                                    \"start_time\",\n",
    "                                    inner_join.userId.cast('bigint').alias('user_id'),\n",
    "                                    \"level\",\n",
    "                                    \"song_id\",\n",
    "                                    \"artist_id\",\n",
    "                                    inner_join.sessionId.alias('session_id'),\n",
    "                                    \"location\",\n",
    "                                    inner_join.userAgent.alias('user_agent')\n",
    "                                    )\n",
    "songplays_table.show(2)\n",
    "songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(table, parquet_path):\n",
    "    table.write.partitionBy(['year', 'month']).parquet(parquet_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "-rw-r--r-- 1 anthelix users    0 Mar 22 02:31 _SUCCESS\n",
      "drwxr-xr-x 3 anthelix users 4.0K Mar 22 02:31 year=__HIVE_DEFAULT_PARTITION__\n",
      "Parquet Files: 17\n",
      "DataFrame rows: 21\n",
      "DataFrame schema: DataFrame[songplay_id: bigint, start_time: timestamp, user_id: bigint, level: string, song_id: string, artist_id: string, session_id: string, location: string, user_agent: string, year: null, month: null]\n",
      "+-----------+-----------------------+-------+-----+------------------+------------------+----------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|songplay_id|start_time             |user_id|level|song_id           |artist_id         |session_id|location                               |user_agent                                                                                                                               |year|month|\n",
      "+-----------+-----------------------+-------+-----+------------------+------------------+----------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|8589934592 |2018-11-24 03:48:21.796|88     |paid |SONWXQJ12A8C134D94|ARNF6401187FB57032|888       |Sacramento--Roseville--Arden-Arcade, CA|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"               |null|null |\n",
      "|8589934593 |2018-11-24 12:42:57.796|80     |paid |SOBONFF12A6D4F84D8|ARIK43K1187B9AE54C|903       |Portland-South Portland, ME            |\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"               |null|null |\n",
      "|8589934594 |2018-11-29 20:29:23.796|49     |paid |SOBONFF12A6D4F84D8|ARIK43K1187B9AE54C|1041      |San Francisco-Oakland-Hayward, CA      |Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0                                                                        |null|null |\n",
      "|8589934595 |2018-11-19 15:36:04.796|49     |paid |SOFSOCN12A8C143F5D|ARXR32B1187FB57099|724       |San Francisco-Oakland-Hayward, CA      |Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0                                                                        |null|null |\n",
      "|8589934596 |2018-11-19 20:44:01.796|42     |paid |SOFFKZS12AB017F194|ARBEBBY1187B9B43DB|632       |New York-Newark-Jersey City, NY-NJ-PA  |\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"                          |null|null |\n",
      "|8589934597 |2018-11-19 22:28:42.796|25     |paid |SOFFKZS12AB017F194|ARBEBBY1187B9B43DB|594       |Marinette, WI-MI                       |\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"                          |null|null |\n",
      "|8589934598 |2018-11-26 17:57:10.796|29     |paid |SOQVMXR12A81C21483|ARKULSX1187FB45F84|924       |Atlanta-Sandy Springs-Roswell, GA      |\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.78.2 (KHTML, like Gecko) Version/7.0.6 Safari/537.78.2\"                  |null|null |\n",
      "|0          |2018-11-15 20:32:47.796|44     |paid |SOBONFF12A6D4F84D8|ARIK43K1187B9AE54C|619       |Waterloo-Cedar Falls, IA               |Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Firefox/31.0                                                        |null|null |\n",
      "|1          |2018-11-21 21:56:47.796|15     |paid |SOZCTXZ12AB0182364|AR5KOSW1187FB35FF4|818       |Chicago-Naperville-Elgin, IL-IN-WI     |\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"|null|null |\n",
      "|2          |2018-11-14 13:11:26.796|34     |free |SOWQTQZ12A58A7B63E|ARPFHN61187FB575F6|495       |Milwaukee-Waukesha-West Allis, WI      |Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Firefox/31.0                                                        |null|null |\n",
      "+-----------+-----------------------+-------+-----+------------------+------------------+----------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[songplay_id: bigint, start_time: timestamp, user_id: bigint, level: string, song_id: string, artist_id: string, session_id: string, location: string, user_agent: string, year: null, month: null]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "songplays_table = create_songplays_table(df_log_clean, df_song, time_table)\n",
    "parquet_path = 'output/songplays_table'\n",
    "write_parquet(songplays_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
