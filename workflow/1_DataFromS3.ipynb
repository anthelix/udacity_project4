{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp==1.7.3 in /opt/conda/lib/python3.7/site-packages (1.7.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install spark-nlp==1.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_242\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08)\n",
      "OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)\n",
      "Python 3.7.6\n"
     ]
    }
   ],
   "source": [
    "!java -version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType, StructType, StructField, StringType, FloatType, IntegerType, LongType\n",
    "from pyspark.sql import types as T\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StringType, DataType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parquet(parquet_path):\n",
    "    ! ls 2>&1 -lh $parquet_path | head -10\n",
    "    ! echo 'Parquet Files:' $(ls | wc -l)\n",
    "    table_parquet = spark.read.parquet(parquet_path)\n",
    "    print('DataFrame rows: %d' % table_parquet.count())\n",
    "    print('DataFrame schema: %s' % table_parquet)\n",
    "    table_parquet.show(10, False)\n",
    "    return table_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_timestamp(df):\n",
    "    # convert timestamps to date time from epoch time so we can get hour of the day\n",
    "    get_timestamp = F.udf(lambda x: datetime.fromtimestamp(x/1000), T.TimestampType())\n",
    "    # add a new column `formated_ts` in our dataframe\n",
    "    df_log_copy = df.withColumn(\"formated_ts\", get_timestamp(df.ts))\n",
    "    df_formated = df_log_copy.dropna(subset='ts')\n",
    "    return df_formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_songs_table(df):\n",
    "    table = df_song \\\n",
    "        .select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\") \\\n",
    "        .filter('song_id != \"\" and title != \"\" and artist_id != \"\"') \\\n",
    "        .sort(\"song_id\") \\\n",
    "        .drop_duplicates(['song_id'])\n",
    "    return(table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet_song(table, parquet_path):\n",
    "    table.write.partitionBy(\"year\", \"artist_id\").parquet(parquet_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(table, parquet_path):\n",
    "    table.write.parquet(parquet_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet_time(table, parquet_path):\n",
    "    table.write.partitionBy(['year', 'month']).parquet(parquet_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET AWS KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIATE SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "        Create or load a Spark session\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS SONG DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_song = \"s3a://udacity-dend/song_data/A/A/A/*.json\"\n",
    "#df =spark.read.format(\"json\").load(song_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data):\n",
    "    '''\n",
    "    process song data\n",
    "    \n",
    "    return df_song\n",
    "    '''\n",
    "    # read  data file\n",
    "    song_schema = StructType([\n",
    "        StructField(\"num_songs\", IntegerType()),\n",
    "        StructField(\"artist_id\", StringType()),\n",
    "        StructField(\"artist_latitude\", FloatType()),\n",
    "        StructField(\"artist_longitude\", FloatType()),\n",
    "        StructField(\"artist_location\", StringType()),\n",
    "        StructField(\"artist_name\", StringType()),\n",
    "        StructField(\"song_id\", StringType()),\n",
    "        StructField(\"title\", StringType()),\n",
    "        StructField(\"duration\", FloatType()),\n",
    "        StructField(\"year\", IntegerType())\n",
    "    ])\n",
    "    \n",
    "    df_song = spark.read.json(input_data, schema = song_schema)\n",
    "    print('DataFrame rows: %d' % df_song.count())\n",
    "    df_song.printSchema()\n",
    "    print('DataFrame schema: %s' % df_song)\n",
    "    return df_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame rows: 24\n",
      "root\n",
      " |-- num_songs: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: float (nullable = true)\n",
      " |-- artist_longitude: float (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: float (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "DataFrame schema: DataFrame[num_songs: int, artist_id: string, artist_latitude: float, artist_longitude: float, artist_location: string, artist_name: string, song_id: string, title: string, duration: float, year: int]\n"
     ]
    }
   ],
   "source": [
    "df_song = process_song_data(spark, input_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_songs_table(df):\n",
    "    table = df_song \\\n",
    "        .select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\") \\\n",
    "        .filter('song_id != \"\" and title != \"\" and artist_id != \"\"') \\\n",
    "        .sort(\"song_id\") \\\n",
    "        .drop_duplicates(['song_id'])\n",
    "    return(table)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56K\n",
      "-rw-r--r--  1 anthelix users    0 Mar 21 18:12 _SUCCESS\n",
      "drwxr-xr-x 11 anthelix users 4.0K Mar 21 18:12 year=0\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 21 18:12 year=1969\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 21 18:12 year=1972\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 21 18:12 year=1978\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 21 18:12 year=1985\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 21 18:12 year=1989\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 21 18:12 year=2000\n",
      "drwxr-xr-x  3 anthelix users 4.0K Mar 21 18:12 year=2001\n",
      "Parquet Files: 20\n",
      "DataFrame rows: 24\n",
      "DataFrame schema: DataFrame[song_id: string, title: string, duration: float, year: int, artist_id: string]\n",
      "+------------------+------------------------------------------------------+---------+----+------------------+\n",
      "|song_id           |title                                                 |duration |year|artist_id         |\n",
      "+------------------+------------------------------------------------------+---------+----+------------------+\n",
      "|SOKTJDS12AF72A25E5|Drown In My Own Tears (24-Bit Digitally Remastered 04)|192.522  |0   |ARA23XO1187B9AF18F|\n",
      "|SOEKAZG12AB018837E|I'll Slap Your Face (Entertainment USA Theme)         |129.85423|2001|ARSVTNL1187B992A91|\n",
      "|SOAFBCP12A8C13CC7D|King Of Scurf (2007 Digital Remaster)                 |301.40036|1972|ARTC1LV1187B9A4858|\n",
      "|SORRNOC12AB017F52B|The Last Beat Of My Heart (b-side)                    |337.81506|2004|ARSZ7L31187FB4E610|\n",
      "|SOQPWCR12A6D4FB2A3|A Poor Recipe For Civic Cohesion                      |118.07302|2005|AR73AIO1187B9AD57B|\n",
      "|SODZYPO12A8C13A91E|Burn My Body (Album Version)                          |177.99791|0   |AR1C2IX1187B99BF74|\n",
      "|SOBRKGM12A8C139EF6|Welcome to the Pleasuredome                           |821.05426|1985|ARXQBR11187B98A2CC|\n",
      "|SOERIDA12A6D4F8506|I Want You (Album Version)                            |192.2869 |2006|ARBZIN01187FB362CC|\n",
      "|SOAPERH12A58A787DC|The One And Only (Edited)                             |230.42567|0   |ARZ5H0P1187B98A1DD|\n",
      "|SOSMJFC12A8C13DE0C|Is That All There Is?                                 |343.87546|0   |AR1KTV21187B9ACD72|\n",
      "+------------------+------------------------------------------------------+---------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[song_id: string, title: string, duration: float, year: int, artist_id: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "songs_table = create_songs_table(df_song)\n",
    "parquet_path = 'output/songs_table'\n",
    "write_parquet_song(songs_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Artists Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artists_table(df):\n",
    "    table = df \\\n",
    "        .selectExpr(\"artist_id\", \"artist_name as name\", \"artist_location as location\", \"artist_latitude as latitude\", \"artist_longitude as longitude\") \\\n",
    "        .filter('artist_id != \"\" and name != \"\"') \\\n",
    "        .sort(\"artist_id\") \\\n",
    "        .drop_duplicates(['artist_id'])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 96K\n",
      "-rw-r--r-- 1 anthelix users 1.5K Mar 21 18:01 part-00000-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.7K Mar 21 18:01 part-00001-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.4K Mar 21 18:01 part-00002-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.6K Mar 21 18:01 part-00003-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.6K Mar 21 18:01 part-00004-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.7K Mar 21 18:01 part-00005-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.5K Mar 21 18:01 part-00006-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.7K Mar 21 18:01 part-00007-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users 1.5K Mar 21 18:01 part-00008-ad8067ba-35b4-4b07-90d3-07a73191c217-c000.snappy.parquet\n",
      "Parquet Files: 20\n",
      "DataFrame rows: 24\n",
      "DataFrame schema: DataFrame[artist_id: string, name: string, location: string, latitude: float, longitude: float]\n",
      "+------------------+-------------------------+---------------------------------+--------+----------+\n",
      "|artist_id         |name                     |location                         |latitude|longitude |\n",
      "+------------------+-------------------------+---------------------------------+--------+----------+\n",
      "|ARTC1LV1187B9A4858|The Bonzo Dog Band       |Goldsmith's College, Lewisham, Lo|51.4536 |-0.01802  |\n",
      "|AR10USD1187B99F3F1|Tweeterfriendly Music    |Burlington, Ontario, Canada      |null    |null      |\n",
      "|ARA23XO1187B9AF18F|The Smithereens          |Carteret, New Jersey             |40.57885|-74.21956 |\n",
      "|AR73AIO1187B9AD57B|Western Addiction        |San Francisco, CA                |37.77916|-122.42005|\n",
      "|ARXQBR11187B98A2CC|Frankie Goes To Hollywood|Liverpool, England               |null    |null      |\n",
      "|ARSVTNL1187B992A91|Jonathan King            |London, England                  |51.50632|-0.12714  |\n",
      "|AR5LMPY1187FB573FE|Chaka Khan_ Rufus        |Chicago, IL                      |41.88415|-87.63241 |\n",
      "|ARZ5H0P1187B98A1DD|Snoop Dogg               |Long Beach, CA                   |33.76672|-118.1924 |\n",
      "|ARCLYBR1187FB53913|Neal Schon               |San Mateo, CA                    |37.54703|-122.31483|\n",
      "|AR1KTV21187B9ACD72|Cristina                 |California - LA                  |34.05349|-118.24532|\n",
      "+------------------+-------------------------+---------------------------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[artist_id: string, name: string, location: string, latitude: float, longitude: float]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "artists_table = create_artists_table(df_song)\n",
    "parquet_path = 'output/artists_table'\n",
    "write_parquet(artists_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOG_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_log = \"s3a://udacity-dend/log_data/*/*/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_log):\n",
    "    '''\n",
    "    process log data\n",
    "    \n",
    "    return df_log\n",
    "    '''\n",
    "    # read log data file\n",
    "    log_schema = StructType([\n",
    "        StructField(\"artist\", StringType()),\n",
    "        StructField(\"auth\", StringType()),\n",
    "        StructField(\"firstName\", StringType()),\n",
    "        StructField(\"gender\", StringType()),\n",
    "        StructField(\"itemInSession\", IntegerType()),\n",
    "        StructField(\"lastName\", StringType()),\n",
    "        StructField(\"length\", FloatType()),    \n",
    "        StructField(\"level\", StringType()),\n",
    "        StructField(\"location\", StringType()),\n",
    "        StructField(\"method\", StringType()),\n",
    "        StructField(\"page\", StringType()),\n",
    "        StructField(\"registration\", FloatType()),\n",
    "        StructField(\"sessionId\", StringType()),\n",
    "        StructField(\"song\", StringType()),\n",
    "        StructField(\"status\", IntegerType()),\n",
    "        StructField(\"ts\", LongType()),\n",
    "        StructField(\"userAgent\", StringType()),\n",
    "        StructField(\"userId\", StringType())\n",
    "    ])\n",
    "    \n",
    "    df_log_raw = spark.read.json(input_log, schema = log_schema)\n",
    "    df_log_next = df_log_raw.filter(\"page='NextSong'\")\n",
    "    df_log_clean=clean_timestamp(df_log_next)\n",
    "    \n",
    "    print('DataFrame rows: %d' % df_log_clean.count())\n",
    "    df_log_clean.printSchema()\n",
    "    print('DataFrame schema: %s' % df_log_clean)\n",
    "    return df_log_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame rows: 6820\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: float (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: float (nullable = true)\n",
      " |-- sessionId: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- formated_ts: timestamp (nullable = true)\n",
      "\n",
      "DataFrame schema: DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: int, lastName: string, length: float, level: string, location: string, method: string, page: string, registration: float, sessionId: string, song: string, status: int, ts: bigint, userAgent: string, userId: string, formated_ts: timestamp]\n"
     ]
    }
   ],
   "source": [
    "df_log_clean = process_log_data(spark, input_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create users table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_users_table(df):\n",
    "    users_table = df \\\n",
    "        .filter('level != \"\"') \\\n",
    "        .orderBy(\"ts\", ascending = False) \\\n",
    "        .coalesce(1)\\\n",
    "        .selectExpr(\"cast(userId as Long) user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender\", \"level\")\\\n",
    "        .drop_duplicates(subset = ['user_id'])\n",
    "    return users_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "-rw-r--r-- 1 anthelix users 3.5K Mar 21 18:01 part-00000-8cae0843-69cd-4e54-ae27-29cbc1c121d8-c000.snappy.parquet\n",
      "-rw-r--r-- 1 anthelix users    0 Mar 21 18:01 _SUCCESS\n",
      "Parquet Files: 20\n",
      "DataFrame rows: 96\n",
      "DataFrame schema: DataFrame[user_id: bigint, first_name: string, last_name: string, gender: string, level: string]\n",
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|2      |Jizelle   |Benjamin |F     |free |\n",
      "|3      |Isaac     |Valdez   |M     |free |\n",
      "|4      |Alivia    |Terrell  |F     |free |\n",
      "|5      |Elijah    |Davis    |M     |free |\n",
      "|6      |Cecilia   |Owens    |F     |free |\n",
      "|7      |Adelyn    |Jordan   |F     |free |\n",
      "|8      |Kaylee    |Summers  |F     |free |\n",
      "|9      |Wyatt     |Scott    |M     |free |\n",
      "|10     |Sylvie    |Cruz     |F     |free |\n",
      "|11     |Christian |Porter   |F     |free |\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: bigint, first_name: string, last_name: string, gender: string, level: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "users_table = create_users_table(df_log_clean)\n",
    "parquet_path = 'output/users_table'\n",
    "write_parquet(users_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_table(df):\n",
    "    time_df = df.select(\n",
    "        col('formated_ts').alias(\"start_time\"),\n",
    "        hour(col('formated_ts')).alias('hour'),\n",
    "        dayofmonth(col('formated_ts')).alias('day'),\n",
    "        weekofyear(col('formated_ts')).alias('week'),\n",
    "        month(col('formated_ts')).alias('month'),\n",
    "        year(col('formated_ts')).alias('year')    \n",
    "    ).drop_duplicates(['start_time'])\n",
    "    print(time_df.show(2))\n",
    "    print(\"ici1\")\n",
    "    time_table = time_df.withColumn('hour', F.hour('start_time')) \\\n",
    "                    .withColumn('day', F.dayofmonth('start_time')) \\\n",
    "                    .withColumn('year', F.year('start_time')) \\\n",
    "                    .withColumn('week', F.weekofyear('start_time')) \\\n",
    "                    .withColumn('month', F.month('start_time')) \\\n",
    "                    .withColumn('weekday', F.dayofweek('start_time').cast(\"string\"))\n",
    "    return time_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+----+-----+----+\n",
      "|          start_time|hour|day|week|month|year|\n",
      "+--------------------+----+---+----+-----+----+\n",
      "|2018-11-21 06:18:...|   6| 21|  47|   11|2018|\n",
      "|2018-11-14 15:20:...|  15| 14|  46|   11|2018|\n",
      "+--------------------+----+---+----+-----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "ici1\n",
      "+--------------------+----+---+----+-----+----+-------+\n",
      "|          start_time|hour|day|week|month|year|weekday|\n",
      "+--------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-21 06:18:...|   6| 21|  47|   11|2018|      4|\n",
      "|2018-11-21 18:49:...|  18| 21|  47|   11|2018|      4|\n",
      "+--------------------+----+---+----+-----+----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_log_clean.show(2)\n",
    "time_table = create_time_table(df_log_clean)\n",
    "time_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+----+-----+----+\n",
      "|          start_time|hour|day|week|month|year|\n",
      "+--------------------+----+---+----+-----+----+\n",
      "|2018-11-21 06:18:...|   6| 21|  47|   11|2018|\n",
      "|2018-11-14 15:20:...|  15| 14|  46|   11|2018|\n",
      "+--------------------+----+---+----+-----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "ici1\n",
      "total 4.0K\n",
      "-rw-r--r-- 1 anthelix users    0 Mar 21 18:02 _SUCCESS\n",
      "drwxr-xr-x 3 anthelix users 4.0K Mar 21 18:02 year=2018\n",
      "Parquet Files: 20\n",
      "DataFrame rows: 6813\n",
      "DataFrame schema: DataFrame[start_time: timestamp, hour: int, day: int, week: int, weekday: string, year: int, month: int]\n",
      "+-----------------------+----+---+----+-------+----+-----+\n",
      "|start_time             |hour|day|week|weekday|year|month|\n",
      "+-----------------------+----+---+----+-------+----+-----+\n",
      "|2018-11-15 16:36:45.796|16  |15 |46  |5      |2018|11   |\n",
      "|2018-11-15 19:02:26.796|19  |15 |46  |5      |2018|11   |\n",
      "|2018-11-21 15:26:35.796|15  |21 |47  |4      |2018|11   |\n",
      "|2018-11-21 17:55:11.796|17  |21 |47  |4      |2018|11   |\n",
      "|2018-11-21 18:49:29.796|18  |21 |47  |4      |2018|11   |\n",
      "|2018-11-14 09:29:50.796|9   |14 |46  |4      |2018|11   |\n",
      "|2018-11-28 18:39:53.796|18  |28 |48  |4      |2018|11   |\n",
      "|2018-11-28 22:46:07.796|22  |28 |48  |4      |2018|11   |\n",
      "|2018-11-05 12:26:13.796|12  |5  |45  |2      |2018|11   |\n",
      "|2018-11-05 14:39:43.796|14  |5  |45  |2      |2018|11   |\n",
      "+-----------------------+----+---+----+-------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[start_time: timestamp, hour: int, day: int, week: int, weekday: string, year: int, month: int]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "time_table = create_time_table(df_log_clean)\n",
    "parquet_path = 'output/time_table'\n",
    "write_parquet_time(time_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the songplays fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_songplays_table(tl, ts,tt):\n",
    "    tl = df_log_clean.alias('tl')\n",
    "    ts = df_song.alias('ts')\n",
    "    \n",
    "    inner_join = tl.join(ts, ((tl.artist == ts.artist_name) & (tl.artist == ts.artist_name)), how='inner')    \n",
    "    songplays = inner_join.withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "    \n",
    "    songplays_table = songplays.selectExpr(\"songplay_id\",\n",
    "                                    \"formated_ts as start_time\",\n",
    "                                    \"cast(userId as Long) user_id\",\n",
    "                                    \"level\",\n",
    "                                    \"song_id\",\n",
    "                                    \"artist_id\",\n",
    "                                    \"sessionId as session_id\",\n",
    "                                    \"location\",\n",
    "                                    \"userAgent as user_agent\") \\\n",
    "                                .withColumn('year', F.year('start_time')) \\\n",
    "                                .withColumn('month', F.month('start_time'))\n",
    "\n",
    "    return songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "-rw-r--r-- 1 anthelix users    0 Mar 21 18:15 _SUCCESS\n",
      "drwxr-xr-x 3 anthelix users 4.0K Mar 21 18:15 year=2018\n",
      "Parquet Files: 20\n",
      "DataFrame rows: 10\n",
      "DataFrame schema: DataFrame[songplay_id: bigint, start_time: timestamp, user_id: bigint, level: string, song_id: string, artist_id: string, session_id: string, location: string, user_agent: string, year: int, month: int]\n",
      "+-----------+-----------------------+-------+-----+------------------+------------------+----------+-------------------------------------+---------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|songplay_id|start_time             |user_id|level|song_id           |artist_id         |session_id|location                             |user_agent                                                                                                     |year|month|\n",
      "+-----------+-----------------------+-------+-----+------------------+------------------+----------+-------------------------------------+---------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|17179869184|2018-11-06 20:05:45.796|97     |paid |SOBLFFE12AF72AA5BA|ARJNIUY12298900C91|293       |Lansing-East Lansing, MI             |\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"     |2018|11   |\n",
      "|0          |2018-11-15 16:55:31.796|42     |paid |SONRWUU12AF72A4283|ARGE7G11187FB37E05|404       |New York-Newark-Jersey City, NY-NJ-PA|\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"|2018|11   |\n",
      "|1          |2018-11-21 05:30:19.796|97     |paid |SONRWUU12AF72A4283|ARGE7G11187FB37E05|797       |Lansing-East Lansing, MI             |\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"     |2018|11   |\n",
      "|2          |2018-11-28 16:54:51.796|14     |free |SOIGHOD12A8C13B5A1|ARY589G1187B9A9F4E|929       |Red Bluff, CA                        |Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0                                       |2018|11   |\n",
      "|3          |2018-11-05 02:21:55.796|44     |paid |SONRWUU12AF72A4283|ARGE7G11187FB37E05|237       |Waterloo-Cedar Falls, IA             |Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Firefox/31.0                              |2018|11   |\n",
      "|4          |2018-11-30 13:20:44.796|43     |free |SOXZYWX12A6310ED0C|ARC1IHZ1187FB4E920|618       |San Antonio-New Braunfels, TX        |\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"       |2018|11   |\n",
      "|5          |2018-11-16 16:40:43.796|97     |paid |SOXZYWX12A6310ED0C|ARC1IHZ1187FB4E920|633       |Lansing-East Lansing, MI             |\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"     |2018|11   |\n",
      "|8589934592 |2018-11-20 00:51:41.796|25     |paid |SONRWUU12AF72A4283|ARGE7G11187FB37E05|594       |Marinette, WI-MI                     |\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"|2018|11   |\n",
      "|8589934593 |2018-11-19 15:36:04.796|49     |paid |SOFSOCN12A8C143F5D|ARXR32B1187FB57099|724       |San Francisco-Oakland-Hayward, CA    |Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0                                              |2018|11   |\n",
      "|8589934594 |2018-11-27 17:08:42.796|36     |paid |SORRNOC12AB017F52B|ARSZ7L31187FB4E610|957       |Janesville-Beloit, WI                |\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"       |2018|11   |\n",
      "+-----------+-----------------------+-------+-----+------------------+------------------+----------+-------------------------------------+---------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[songplay_id: bigint, start_time: timestamp, user_id: bigint, level: string, song_id: string, artist_id: string, session_id: string, location: string, user_agent: string, year: int, month: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process and check\n",
    "songplays_table = create_songplays_table(df_log_clean, df_song, time_table)\n",
    "parquet_path = 'output/songplays_table'\n",
    "write_parquet_time(songplays_table, parquet_path)\n",
    "check_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
